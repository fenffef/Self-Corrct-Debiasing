import json
from datetime import datetime
from datasets import load_dataset
from openai import OpenAI
import argparse
from tqdm import tqdm  # è¿›åº¦æ¡

# ===================== é»˜è®¤é…ç½® =====================
DEFAULT_API_MODEL = "gpt-4o-ca"
DEFAULT_BASE_URL = "https://api.chatanywhere.tech/v1"
DEFAULT_MAX_OUTPUT_TOKENS = 512

# ===================== API åˆå§‹åŒ– =====================
def init_client(api_key, base_url):
    return OpenAI(api_key=api_key, base_url=base_url)

# ===================== æ•°æ®é›†åŠ è½½ =====================
def load_ceb_dataset(dataset_id, data_file_path, num_samples=-1):
    split_str = "train" if num_samples == -1 else f"train[:{num_samples}]"
    dataset = load_dataset(dataset_id, data_files=data_file_path, split=split_str)
    return dataset

# ===================== ä¸»å¤„ç†å‡½æ•° =====================
def process_dataset(dataset_id, data_file_path, save_path, client, api_model, num_samples, max_output_tokens, use_api=True):
    dataset = load_ceb_dataset(dataset_id, data_file_path, num_samples)
    total_samples = len(dataset)
    print(f"æˆåŠŸåŠ è½½ {total_samples} ä¸ªæ ·æœ¬")

    # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶
    with open(save_path, "w", encoding="utf-8") as f:
        pass

    for i, item in enumerate(tqdm(dataset, desc="Processing", ncols=100)):
        description = item["prompt"]

        # åªæ‰“å°ç¬¬ä¸€æ¡æ ·æœ¬çš„ prompt
        if i == 0:
            print(f"\nğŸ¯ æ ·æœ¬ {i+1}/{total_samples}")
            print(f"Prompt preview:\n{description[:120]}...")

        if use_api:
            resp = client.responses.create(
                model=api_model,
                input=description,
                reasoning={"include_outputs": True},
                max_output_tokens=max_output_tokens,
            )
            generated = resp.output_text
        else:
            generated = "<think>æœ¬åœ°æ¨¡å‹æ¨ç†ç»“æœ</think> <bias>Yes/No</bias>"

        record = {
            "id": i,
            "prompt": description,
            "response": generated,
            "model": api_model if use_api else "local_model",
            "task": data_file_path  # ç”¨æ–‡ä»¶åæ ‡è¯†ä»»åŠ¡
        }

        with open(save_path, "a", encoding="utf-8") as fw:
            fw.write(json.dumps(record, ensure_ascii=False) + "\n")

    print("\nâœ… å…¨éƒ¨å®Œæˆ")
    print(f"è¾“å‡ºç»“æœä¿å­˜åœ¨: {save_path}")

# ===================== å‘½ä»¤è¡Œå‚æ•° =====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CEB æ•°æ®é›†ç»­å†™å¤„ç†è„šæœ¬")
    parser.add_argument("--dataset_id", type=str, required=True, help="HuggingFace æ•°æ®é›† ID")
    parser.add_argument("--data_file", type=str, required=True, help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--save_path", type=str, required=True, help="è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--api_key", type=str, required=True, help="OpenAI/ChatAnywhere API Key")
    parser.add_argument("--api_model", type=str, default=DEFAULT_API_MODEL, help="è°ƒç”¨æ¨¡å‹åç§°")
    parser.add_argument("--num_samples", type=int, default=-1, help="å¤„ç†æ ·æœ¬æ•°é‡ï¼Œ-1 è¡¨ç¤ºå…¨éƒ¨")
    parser.add_argument("--max_output_tokens", type=int, default=DEFAULT_MAX_OUTPUT_TOKENS, help="æ¯æ¡è¾“å‡ºæœ€å¤§ token")
    parser.add_argument("--use_api", type=int, default=1, help="æ˜¯å¦ä½¿ç”¨ APIï¼Œ1 ä¸º Trueï¼Œ0 ä¸º False")

    args = parser.parse_args()

    client = init_client(api_key=args.api_key, base_url=DEFAULT_BASE_URL)

    process_dataset(
        dataset_id=args.dataset_id,
        data_file_path=args.data_file,
        save_path=args.save_path,
        client=client,
        api_model=args.api_model,
        num_samples=args.num_samples,
        max_output_tokens=args.max_output_tokens,
        use_api=bool(args.use_api)
    )